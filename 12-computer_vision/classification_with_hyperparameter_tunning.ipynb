{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"c:/Data-Science/12-computer_vision/cancer_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>28.11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>39.28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>188.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>2501.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.16340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.34540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.42680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.20120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.30400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>2.87300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>4.88500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>21.98000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>542.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.03113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.39600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.05279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.07895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.02984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>36.04000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>49.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>251.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>4254.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.22260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>1.05800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>1.25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.29100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.66380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benign_0__mal_1</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.627417</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count        mean         std         min  \\\n",
       "mean radius              569.0   14.127292    3.524049    6.981000   \n",
       "mean texture             569.0   19.289649    4.301036    9.710000   \n",
       "mean perimeter           569.0   91.969033   24.298981   43.790000   \n",
       "mean area                569.0  654.889104  351.914129  143.500000   \n",
       "mean smoothness          569.0    0.096360    0.014064    0.052630   \n",
       "mean compactness         569.0    0.104341    0.052813    0.019380   \n",
       "mean concavity           569.0    0.088799    0.079720    0.000000   \n",
       "mean concave points      569.0    0.048919    0.038803    0.000000   \n",
       "mean symmetry            569.0    0.181162    0.027414    0.106000   \n",
       "mean fractal dimension   569.0    0.062798    0.007060    0.049960   \n",
       "radius error             569.0    0.405172    0.277313    0.111500   \n",
       "texture error            569.0    1.216853    0.551648    0.360200   \n",
       "perimeter error          569.0    2.866059    2.021855    0.757000   \n",
       "area error               569.0   40.337079   45.491006    6.802000   \n",
       "smoothness error         569.0    0.007041    0.003003    0.001713   \n",
       "compactness error        569.0    0.025478    0.017908    0.002252   \n",
       "concavity error          569.0    0.031894    0.030186    0.000000   \n",
       "concave points error     569.0    0.011796    0.006170    0.000000   \n",
       "symmetry error           569.0    0.020542    0.008266    0.007882   \n",
       "fractal dimension error  569.0    0.003795    0.002646    0.000895   \n",
       "worst radius             569.0   16.269190    4.833242    7.930000   \n",
       "worst texture            569.0   25.677223    6.146258   12.020000   \n",
       "worst perimeter          569.0  107.261213   33.602542   50.410000   \n",
       "worst area               569.0  880.583128  569.356993  185.200000   \n",
       "worst smoothness         569.0    0.132369    0.022832    0.071170   \n",
       "worst compactness        569.0    0.254265    0.157336    0.027290   \n",
       "worst concavity          569.0    0.272188    0.208624    0.000000   \n",
       "worst concave points     569.0    0.114606    0.065732    0.000000   \n",
       "worst symmetry           569.0    0.290076    0.061867    0.156500   \n",
       "worst fractal dimension  569.0    0.083946    0.018061    0.055040   \n",
       "benign_0__mal_1          569.0    0.627417    0.483918    0.000000   \n",
       "\n",
       "                                25%         50%          75%         max  \n",
       "mean radius               11.700000   13.370000    15.780000    28.11000  \n",
       "mean texture              16.170000   18.840000    21.800000    39.28000  \n",
       "mean perimeter            75.170000   86.240000   104.100000   188.50000  \n",
       "mean area                420.300000  551.100000   782.700000  2501.00000  \n",
       "mean smoothness            0.086370    0.095870     0.105300     0.16340  \n",
       "mean compactness           0.064920    0.092630     0.130400     0.34540  \n",
       "mean concavity             0.029560    0.061540     0.130700     0.42680  \n",
       "mean concave points        0.020310    0.033500     0.074000     0.20120  \n",
       "mean symmetry              0.161900    0.179200     0.195700     0.30400  \n",
       "mean fractal dimension     0.057700    0.061540     0.066120     0.09744  \n",
       "radius error               0.232400    0.324200     0.478900     2.87300  \n",
       "texture error              0.833900    1.108000     1.474000     4.88500  \n",
       "perimeter error            1.606000    2.287000     3.357000    21.98000  \n",
       "area error                17.850000   24.530000    45.190000   542.20000  \n",
       "smoothness error           0.005169    0.006380     0.008146     0.03113  \n",
       "compactness error          0.013080    0.020450     0.032450     0.13540  \n",
       "concavity error            0.015090    0.025890     0.042050     0.39600  \n",
       "concave points error       0.007638    0.010930     0.014710     0.05279  \n",
       "symmetry error             0.015160    0.018730     0.023480     0.07895  \n",
       "fractal dimension error    0.002248    0.003187     0.004558     0.02984  \n",
       "worst radius              13.010000   14.970000    18.790000    36.04000  \n",
       "worst texture             21.080000   25.410000    29.720000    49.54000  \n",
       "worst perimeter           84.110000   97.660000   125.400000   251.20000  \n",
       "worst area               515.300000  686.500000  1084.000000  4254.00000  \n",
       "worst smoothness           0.116600    0.131300     0.146000     0.22260  \n",
       "worst compactness          0.147200    0.211900     0.339100     1.05800  \n",
       "worst concavity            0.114500    0.226700     0.382900     1.25200  \n",
       "worst concave points       0.064930    0.099930     0.161400     0.29100  \n",
       "worst symmetry             0.250400    0.282200     0.317900     0.66380  \n",
       "worst fractal dimension    0.071460    0.080040     0.092080     0.20750  \n",
       "benign_0__mal_1            0.000000    1.000000     1.000000     1.00000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkl-random 1.0.1 requires cython, which is not installed.\n",
      "tensorflow 1.10.0 has requirement numpy<=1.14.5,>=1.13.3, but you'll have numpy 1.19.5 which is incompatible.\n",
      "tensorflow 1.10.0 has requirement setuptools<=39.1.0, but you'll have setuptools 58.0.4 which is incompatible.\n",
      "You are using pip version 10.0.1, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://files.pythonhosted.org/packages/10/5b/0479d7d845b5ba410ca702ffcd7f2cd95a14a4dfff1fde2637802b258b9b/seaborn-0.11.2-py3-none-any.whl (292kB)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from seaborn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from seaborn) (1.1.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2920b02db00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='benign_0__mal_1',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('benign_0__mal_1',axis=1).values\n",
    "y=df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "MinMaxScaler(copy=True,feature_range=(0,1))\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for a binary classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Yashraj\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Sequential()\n",
    "\n",
    "model.add(Dense(units=30,activation=\"relu\"))\n",
    "\n",
    "\n",
    "model.add(Dense(units=15,activation=\"relu\"))\n",
    "\n",
    "\n",
    "model.add(Dense(units=1,activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "#for a binary classification problem\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 1s 2ms/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 62us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 57us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 63us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 46us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 46us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 50us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 52us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 44us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 46us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 43us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 40us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 63us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 46us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 57us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 52us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 46us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 43us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 52us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 44us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 61us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 52us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 42us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 46us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 44us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 42us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 41us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 80/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 52us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 77us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 52us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 57us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 57us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 46us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 50us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 44us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 46us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 118/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 119/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 120/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 121/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 122/600\n",
      "426/426 [==============================] - 0s 57us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 123/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 124/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 125/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 126/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 127/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 128/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 129/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 130/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 131/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 132/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 133/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 134/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 135/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 136/600\n",
      "426/426 [==============================] - 0s 61us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 137/600\n",
      "426/426 [==============================] - 0s 58us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 138/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 139/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 140/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 141/600\n",
      "426/426 [==============================] - 0s 60us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 142/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 143/600\n",
      "426/426 [==============================] - 0s 60us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 144/600\n",
      "426/426 [==============================] - 0s 60us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 145/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 146/600\n",
      "426/426 [==============================] - 0s 87us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 147/600\n",
      "426/426 [==============================] - 0s 81us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 148/600\n",
      "426/426 [==============================] - 0s 86us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 149/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 150/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 151/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 152/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 153/600\n",
      "426/426 [==============================] - 0s 101us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 154/600\n",
      "426/426 [==============================] - 0s 89us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 155/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 156/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 157/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 158/600\n",
      "426/426 [==============================] - 0s 106us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 159/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 124us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 160/600\n",
      "426/426 [==============================] - 0s 103us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 161/600\n",
      "426/426 [==============================] - 0s 126us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 162/600\n",
      "426/426 [==============================] - 0s 117us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 163/600\n",
      "426/426 [==============================] - 0s 106us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 164/600\n",
      "426/426 [==============================] - 0s 109us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 165/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 166/600\n",
      "426/426 [==============================] - 0s 111us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 167/600\n",
      "426/426 [==============================] - 0s 105us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 168/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 169/600\n",
      "426/426 [==============================] - 0s 87us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 170/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 171/600\n",
      "426/426 [==============================] - 0s 97us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 172/600\n",
      "426/426 [==============================] - 0s 83us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 173/600\n",
      "426/426 [==============================] - 0s 92us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 174/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 175/600\n",
      "426/426 [==============================] - 0s 84us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 176/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 177/600\n",
      "426/426 [==============================] - 0s 94us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 178/600\n",
      "426/426 [==============================] - 0s 106us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 179/600\n",
      "426/426 [==============================] - 0s 92us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 180/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 181/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 182/600\n",
      "426/426 [==============================] - 0s 98us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 183/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 184/600\n",
      "426/426 [==============================] - 0s 90us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 185/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 186/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 187/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 188/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 189/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 190/600\n",
      "426/426 [==============================] - 0s 90us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 191/600\n",
      "426/426 [==============================] - 0s 89us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 192/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 193/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 194/600\n",
      "426/426 [==============================] - 0s 87us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 195/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 196/600\n",
      "426/426 [==============================] - 0s 94us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 197/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 198/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 199/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 200/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 201/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 202/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 203/600\n",
      "426/426 [==============================] - 0s 92us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 204/600\n",
      "426/426 [==============================] - 0s 92us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 205/600\n",
      "426/426 [==============================] - 0s 89us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 206/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 207/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 208/600\n",
      "426/426 [==============================] - 0s 90us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 209/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 210/600\n",
      "426/426 [==============================] - 0s 77us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 211/600\n",
      "426/426 [==============================] - 0s 94us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 212/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 213/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 214/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 215/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 216/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 217/600\n",
      "426/426 [==============================] - 0s 115us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 218/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 219/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 220/600\n",
      "426/426 [==============================] - 0s 100us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 221/600\n",
      "426/426 [==============================] - 0s 100us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 222/600\n",
      "426/426 [==============================] - 0s 63us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 223/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 224/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 225/600\n",
      "426/426 [==============================] - 0s 84us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 226/600\n",
      "426/426 [==============================] - 0s 90us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 227/600\n",
      "426/426 [==============================] - 0s 106us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 228/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 229/600\n",
      "426/426 [==============================] - 0s 83us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 230/600\n",
      "426/426 [==============================] - 0s 90us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 231/600\n",
      "426/426 [==============================] - 0s 86us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 232/600\n",
      "426/426 [==============================] - 0s 89us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 233/600\n",
      "426/426 [==============================] - 0s 114us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 234/600\n",
      "426/426 [==============================] - 0s 97us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 235/600\n",
      "426/426 [==============================] - 0s 116us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 236/600\n",
      "426/426 [==============================] - 0s 117us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 237/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 115us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 238/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 239/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 240/600\n",
      "426/426 [==============================] - 0s 112us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 241/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 242/600\n",
      "426/426 [==============================] - 0s 124us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 243/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 244/600\n",
      "426/426 [==============================] - 0s 87us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 245/600\n",
      "426/426 [==============================] - 0s 86us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 246/600\n",
      "426/426 [==============================] - 0s 114us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 247/600\n",
      "426/426 [==============================] - 0s 101us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 248/600\n",
      "426/426 [==============================] - 0s 106us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 249/600\n",
      "426/426 [==============================] - 0s 107us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 250/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 251/600\n",
      "426/426 [==============================] - 0s 97us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 252/600\n",
      "426/426 [==============================] - 0s 107us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 253/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 254/600\n",
      "426/426 [==============================] - 0s 101us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 255/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 256/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 257/600\n",
      "426/426 [==============================] - 0s 104us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 258/600\n",
      "426/426 [==============================] - 0s 105us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 259/600\n",
      "426/426 [==============================] - 0s 102us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 260/600\n",
      "426/426 [==============================] - 0s 101us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 261/600\n",
      "426/426 [==============================] - 0s 89us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 262/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 263/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 264/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 265/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 266/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 267/600\n",
      "426/426 [==============================] - 0s 110us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 268/600\n",
      "426/426 [==============================] - 0s 98us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 269/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 270/600\n",
      "426/426 [==============================] - 0s 109us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 271/600\n",
      "426/426 [==============================] - 0s 101us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 272/600\n",
      "426/426 [==============================] - 0s 89us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 273/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 274/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 275/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 276/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 277/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 278/600\n",
      "426/426 [==============================] - 0s 105us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 279/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 280/600\n",
      "426/426 [==============================] - 0s 103us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 281/600\n",
      "426/426 [==============================] - 0s 98us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 282/600\n",
      "426/426 [==============================] - 0s 98us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 283/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 284/600\n",
      "426/426 [==============================] - 0s 100us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 285/600\n",
      "426/426 [==============================] - 0s 100us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 286/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 287/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 288/600\n",
      "426/426 [==============================] - 0s 97us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 289/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 290/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 291/600\n",
      "426/426 [==============================] - 0s 87us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 292/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 293/600\n",
      "426/426 [==============================] - 0s 107us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 294/600\n",
      "426/426 [==============================] - 0s 100us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 295/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 296/600\n",
      "426/426 [==============================] - 0s 119us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 297/600\n",
      "426/426 [==============================] - 0s 90us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 298/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 299/600\n",
      "426/426 [==============================] - 0s 103us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 300/600\n",
      "426/426 [==============================] - 0s 86us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 301/600\n",
      "426/426 [==============================] - 0s 100us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 302/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 303/600\n",
      "426/426 [==============================] - 0s 108us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 304/600\n",
      "426/426 [==============================] - 0s 97us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 305/600\n",
      "426/426 [==============================] - 0s 117us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 306/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 307/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 308/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 309/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 310/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 311/600\n",
      "426/426 [==============================] - 0s 60us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 312/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 313/600\n",
      "426/426 [==============================] - 0s 92us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 314/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 315/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 316/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 317/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 318/600\n",
      "426/426 [==============================] - 0s 90us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 319/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 320/600\n",
      "426/426 [==============================] - 0s 81us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 321/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 322/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 323/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 324/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 325/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 326/600\n",
      "426/426 [==============================] - 0s 81us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 327/600\n",
      "426/426 [==============================] - 0s 61us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 328/600\n",
      "426/426 [==============================] - 0s 102us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 329/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 330/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 331/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 332/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 333/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 334/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 335/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 336/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 337/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 338/600\n",
      "426/426 [==============================] - 0s 84us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 339/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 340/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 341/600\n",
      "426/426 [==============================] - 0s 58us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 342/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 343/600\n",
      "426/426 [==============================] - 0s 61us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 344/600\n",
      "426/426 [==============================] - 0s 62us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 345/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 346/600\n",
      "426/426 [==============================] - 0s 90us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 347/600\n",
      "426/426 [==============================] - 0s 113us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 348/600\n",
      "426/426 [==============================] - 0s 84us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 349/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 350/600\n",
      "426/426 [==============================] - 0s 81us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 351/600\n",
      "426/426 [==============================] - 0s 94us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 352/600\n",
      "426/426 [==============================] - 0s 81us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 353/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 354/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 355/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 356/600\n",
      "426/426 [==============================] - 0s 73us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 357/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 358/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 359/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 360/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 361/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 362/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 363/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 364/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 365/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 366/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 367/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 368/600\n",
      "426/426 [==============================] - 0s 60us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 369/600\n",
      "426/426 [==============================] - 0s 62us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 370/600\n",
      "426/426 [==============================] - 0s 89us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 371/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 372/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 373/600\n",
      "426/426 [==============================] - 0s 84us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 374/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 375/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 376/600\n",
      "426/426 [==============================] - 0s 84us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 377/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 378/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 379/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 380/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 381/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 382/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 383/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 384/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 385/600\n",
      "426/426 [==============================] - 0s 55us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 386/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 387/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 388/600\n",
      "426/426 [==============================] - 0s 55us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 389/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 390/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 391/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 392/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 393/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 394/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 395/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 396/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 397/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 398/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 399/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 400/600\n",
      "426/426 [==============================] - 0s 57us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 401/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 402/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 403/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 404/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 405/600\n",
      "426/426 [==============================] - 0s 84us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 406/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 407/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 408/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 409/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 410/600\n",
      "426/426 [==============================] - 0s 73us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 411/600\n",
      "426/426 [==============================] - 0s 63us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 412/600\n",
      "426/426 [==============================] - 0s 58us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 413/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 414/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 415/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 416/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 417/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 418/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 419/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 420/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 421/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 422/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 423/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 424/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 425/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 426/600\n",
      "426/426 [==============================] - 0s 87us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 427/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 428/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 429/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 430/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 431/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 432/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 433/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 434/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 435/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 436/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 437/600\n",
      "426/426 [==============================] - 0s 61us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 438/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 439/600\n",
      "426/426 [==============================] - 0s 81us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 440/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 441/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 442/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 443/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 444/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 445/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 446/600\n",
      "426/426 [==============================] - 0s 73us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 447/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 448/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 449/600\n",
      "426/426 [==============================] - 0s 81us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 450/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 451/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 452/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 453/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 454/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 455/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 456/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 457/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 458/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 459/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 460/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 461/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 462/600\n",
      "426/426 [==============================] - 0s 63us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 463/600\n",
      "426/426 [==============================] - 0s 63us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 464/600\n",
      "426/426 [==============================] - 0s 73us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 465/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 466/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 467/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 468/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 469/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 470/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 471/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 472/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/600\n",
      "426/426 [==============================] - 0s 63us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 474/600\n",
      "426/426 [==============================] - 0s 61us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 475/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 476/600\n",
      "426/426 [==============================] - 0s 55us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 477/600\n",
      "426/426 [==============================] - 0s 58us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 478/600\n",
      "426/426 [==============================] - 0s 61us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 479/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 480/600\n",
      "426/426 [==============================] - 0s 62us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 481/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 482/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 483/600\n",
      "426/426 [==============================] - 0s 67us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 484/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 485/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 486/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 487/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 488/600\n",
      "426/426 [==============================] - 0s 55us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 489/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 490/600\n",
      "426/426 [==============================] - 0s 78us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 491/600\n",
      "426/426 [==============================] - 0s 83us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 492/600\n",
      "426/426 [==============================] - 0s 87us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 493/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 494/600\n",
      "426/426 [==============================] - 0s 58us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 495/600\n",
      "426/426 [==============================] - 0s 100us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 496/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 497/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 498/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 499/600\n",
      "426/426 [==============================] - 0s 63us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 500/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 501/600\n",
      "426/426 [==============================] - 0s 55us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 502/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 503/600\n",
      "426/426 [==============================] - 0s 73us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 504/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 505/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 506/600\n",
      "426/426 [==============================] - 0s 77us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 507/600\n",
      "426/426 [==============================] - 0s 99us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 508/600\n",
      "426/426 [==============================] - 0s 94us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 509/600\n",
      "426/426 [==============================] - 0s 103us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 510/600\n",
      "426/426 [==============================] - 0s 100us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 511/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 512/600\n",
      "426/426 [==============================] - 0s 108us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 513/600\n",
      "426/426 [==============================] - 0s 103us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 514/600\n",
      "426/426 [==============================] - 0s 103us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 515/600\n",
      "426/426 [==============================] - 0s 87us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 516/600\n",
      "426/426 [==============================] - 0s 87us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 517/600\n",
      "426/426 [==============================] - 0s 100us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 518/600\n",
      "426/426 [==============================] - 0s 106us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 519/600\n",
      "426/426 [==============================] - 0s 97us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 520/600\n",
      "426/426 [==============================] - 0s 101us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 521/600\n",
      "426/426 [==============================] - 0s 113us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 522/600\n",
      "426/426 [==============================] - 0s 97us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 523/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 524/600\n",
      "426/426 [==============================] - 0s 91us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 525/600\n",
      "426/426 [==============================] - 0s 106us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 526/600\n",
      "426/426 [==============================] - 0s 92us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 527/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 528/600\n",
      "426/426 [==============================] - 0s 79us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 529/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 530/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 531/600\n",
      "426/426 [==============================] - 0s 101us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 532/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 533/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 534/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 535/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 536/600\n",
      "426/426 [==============================] - 0s 84us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 537/600\n",
      "426/426 [==============================] - 0s 94us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 538/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 539/600\n",
      "426/426 [==============================] - 0s 62us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 540/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 541/600\n",
      "426/426 [==============================] - 0s 92us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 542/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 543/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 544/600\n",
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 545/600\n",
      "426/426 [==============================] - 0s 60us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 546/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 547/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 548/600\n",
      "426/426 [==============================] - 0s 81us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 549/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 550/600\n",
      "426/426 [==============================] - 0s 73us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 551/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 82us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 552/600\n",
      "426/426 [==============================] - 0s 93us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 553/600\n",
      "426/426 [==============================] - 0s 107us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 554/600\n",
      "426/426 [==============================] - 0s 106us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 555/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 556/600\n",
      "426/426 [==============================] - 0s 102us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 557/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 558/600\n",
      "426/426 [==============================] - 0s 77us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 559/600\n",
      "426/426 [==============================] - 0s 61us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 560/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 561/600\n",
      "426/426 [==============================] - 0s 57us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 562/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 563/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 564/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 565/600\n",
      "426/426 [==============================] - 0s 85us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 566/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 567/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 568/600\n",
      "426/426 [==============================] - 0s 72us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 569/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 570/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 571/600\n",
      "426/426 [==============================] - 0s 106us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 572/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 573/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 574/600\n",
      "426/426 [==============================] - 0s 73us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 575/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 576/600\n",
      "426/426 [==============================] - 0s 58us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 577/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 578/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 579/600\n",
      "426/426 [==============================] - 0s 75us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 580/600\n",
      "426/426 [==============================] - 0s 83us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 581/600\n",
      "426/426 [==============================] - 0s 70us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 582/600\n",
      "426/426 [==============================] - 0s 71us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 583/600\n",
      "426/426 [==============================] - 0s 77us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 584/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 585/600\n",
      "426/426 [==============================] - 0s 63us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 586/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 587/600\n",
      "426/426 [==============================] - 0s 88us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 588/600\n",
      "426/426 [==============================] - 0s 96us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 589/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 590/600\n",
      "426/426 [==============================] - 0s 86us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 591/600\n",
      "426/426 [==============================] - 0s 74us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 592/600\n",
      "426/426 [==============================] - 0s 76us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 593/600\n",
      "426/426 [==============================] - 0s 86us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 594/600\n",
      "426/426 [==============================] - 0s 77us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 595/600\n",
      "426/426 [==============================] - 0s 84us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 596/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 597/600\n",
      "426/426 [==============================] - 0s 60us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 598/600\n",
      "426/426 [==============================] - 0s 57us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 599/600\n",
      "426/426 [==============================] - 0s 69us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 600/600\n",
      "426/426 [==============================] - 0s 73us/step - loss: 5.8755 - val_loss: 6.1317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2920f97d0b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# traininf the model\n",
    "model.fit(x=X_train,\n",
    "         y=y_train,\n",
    "         epochs=600,\n",
    "         validation_data=(X_test,y_test),verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329,\n",
       "  6.131686667462329],\n",
       " 'loss': [5.87547999368587,\n",
       "  5.875480018311263,\n",
       "  5.875480058607361,\n",
       "  5.875480047414,\n",
       "  5.87547998249251,\n",
       "  5.87547998249251,\n",
       "  5.8754799802538376,\n",
       "  5.875480065323377,\n",
       "  5.875480018311263,\n",
       "  5.875479975776493,\n",
       "  5.875480011595247,\n",
       "  5.875480029504624,\n",
       "  5.875480011595247,\n",
       "  5.875480029504624,\n",
       "  5.875479975776493,\n",
       "  5.87547999368587,\n",
       "  5.8754800541300165,\n",
       "  5.875479975776493,\n",
       "  5.875480087710098,\n",
       "  5.87547999368587,\n",
       "  5.875480016072591,\n",
       "  5.87547999368587,\n",
       "  5.87547999368587,\n",
       "  5.875479946673756,\n",
       "  5.875480029504624,\n",
       "  5.875480016072591,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.8754799948052066,\n",
       "  5.87547999368587,\n",
       "  5.875479957867117,\n",
       "  5.875480065323377,\n",
       "  5.875479975776493,\n",
       "  5.875480029504624,\n",
       "  5.875479975776493,\n",
       "  5.875480029504624,\n",
       "  5.875479998163215,\n",
       "  5.875480011595247,\n",
       "  5.875479946673756,\n",
       "  5.875480065323377,\n",
       "  5.8754800541300165,\n",
       "  5.875480016072591,\n",
       "  5.875480016072591,\n",
       "  5.875480000401886,\n",
       "  5.87547999368587,\n",
       "  5.875479992006867,\n",
       "  5.875480033981968,\n",
       "  5.875479946673756,\n",
       "  5.8754799220483624,\n",
       "  5.875480029504624,\n",
       "  5.875480011595247,\n",
       "  5.8754799399577395,\n",
       "  5.87547998249251,\n",
       "  5.875479969060477,\n",
       "  5.8754799399577395,\n",
       "  5.875479941077075,\n",
       "  5.875479941077075,\n",
       "  5.875479941077075,\n",
       "  5.8754799220483624,\n",
       "  5.875480029504624,\n",
       "  5.8754800429366565,\n",
       "  5.875480016072591,\n",
       "  5.875480033981968,\n",
       "  5.875480011595247,\n",
       "  5.875480048533337,\n",
       "  5.87547999368587,\n",
       "  5.875479957867117,\n",
       "  5.875480065323377,\n",
       "  5.8754799511511,\n",
       "  5.875480012714583,\n",
       "  5.87547999368587,\n",
       "  5.8754799802538376,\n",
       "  5.875479957867117,\n",
       "  5.875480051891345,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.87547999368587,\n",
       "  5.87547998249251,\n",
       "  5.875480029504624,\n",
       "  5.8754799399577395,\n",
       "  5.8754799399577395,\n",
       "  5.875480029504624,\n",
       "  5.875479886229609,\n",
       "  5.875480065323377,\n",
       "  5.875480016072591,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.875480004879231,\n",
       "  5.8754799399577395,\n",
       "  5.875480004879231,\n",
       "  5.875480029504624,\n",
       "  5.875480065323377,\n",
       "  5.8754799220483624,\n",
       "  5.875479944435083,\n",
       "  5.875479969060477,\n",
       "  5.875480000401886,\n",
       "  5.875480011595247,\n",
       "  5.875480029504624,\n",
       "  5.875480051891345,\n",
       "  5.875479941077075,\n",
       "  5.875480029504624,\n",
       "  5.875480018311263,\n",
       "  5.87547999368587,\n",
       "  5.875479975776493,\n",
       "  5.875479957867117,\n",
       "  5.875480065323377,\n",
       "  5.875480065323377,\n",
       "  5.875480018311263,\n",
       "  5.8754799287643795,\n",
       "  5.875479946673756,\n",
       "  5.875480020549936,\n",
       "  5.87547998249251,\n",
       "  5.875480029504624,\n",
       "  5.875479957867117,\n",
       "  5.875480051891345,\n",
       "  5.8754799220483624,\n",
       "  5.8754800278256205,\n",
       "  5.87547989742297,\n",
       "  5.875479998163215,\n",
       "  5.8754799802538376,\n",
       "  5.875479957867117,\n",
       "  5.875479957867117,\n",
       "  5.875480011595247,\n",
       "  5.875480029504624,\n",
       "  5.875480022788607,\n",
       "  5.875479969060477,\n",
       "  5.875480051891345,\n",
       "  5.875480065323377,\n",
       "  5.875479998163215,\n",
       "  5.875480011595247,\n",
       "  5.875480029504624,\n",
       "  5.875480047414,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.875480051891345,\n",
       "  5.875480029504624,\n",
       "  5.875480040697984,\n",
       "  5.875479964583133,\n",
       "  5.8754799802538376,\n",
       "  5.875479957867117,\n",
       "  5.875480011595247,\n",
       "  5.875479904138986,\n",
       "  5.875480033981968,\n",
       "  5.87547999368587,\n",
       "  5.875480004879231,\n",
       "  5.8754799511511,\n",
       "  5.875480011595247,\n",
       "  5.87547999368587,\n",
       "  5.875479975776493,\n",
       "  5.875479957867117,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.8754799768958295,\n",
       "  5.875479944435083,\n",
       "  5.87548003622064,\n",
       "  5.875479957867117,\n",
       "  5.875480029504624,\n",
       "  5.875479975776493,\n",
       "  5.87547999368587,\n",
       "  5.875480047414,\n",
       "  5.875480040697984,\n",
       "  5.87547999368587,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.875480058607361,\n",
       "  5.875480000401886,\n",
       "  5.87547999368587,\n",
       "  5.87547998249251,\n",
       "  5.875479944435083,\n",
       "  5.875479975776493,\n",
       "  5.8754800541300165,\n",
       "  5.875480033981968,\n",
       "  5.875480051891345,\n",
       "  5.875479941077075,\n",
       "  5.875480011595247,\n",
       "  5.875479957867117,\n",
       "  5.875480065323377,\n",
       "  5.875480018311263,\n",
       "  5.875479958986452,\n",
       "  5.875479915332346,\n",
       "  5.875480018311263,\n",
       "  5.875480040697984,\n",
       "  5.87547999368587,\n",
       "  5.875480004879231,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.875480029504624,\n",
       "  5.87547999368587,\n",
       "  5.87547999368587,\n",
       "  5.875480076516738,\n",
       "  5.8754799802538376,\n",
       "  5.875479957867117,\n",
       "  5.875480051891345,\n",
       "  5.87547999368587,\n",
       "  5.875480016072591,\n",
       "  5.875480004879231,\n",
       "  5.875479957867117,\n",
       "  5.875480051891345,\n",
       "  5.875480029504624,\n",
       "  5.875480011595247,\n",
       "  5.875480065323377,\n",
       "  5.875480009916244,\n",
       "  5.875480000401886,\n",
       "  5.875480048533337,\n",
       "  5.875480000401886,\n",
       "  5.875479975776493,\n",
       "  5.875479946673756,\n",
       "  5.875480029504624,\n",
       "  5.87548003062396,\n",
       "  5.8754799802538376,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.8754799768958295,\n",
       "  5.875480029504624,\n",
       "  5.875479946673756,\n",
       "  5.87547998249251,\n",
       "  5.875480004879231,\n",
       "  5.875480087710098,\n",
       "  5.8754799220483624,\n",
       "  5.875480016072591,\n",
       "  5.875480029504624,\n",
       "  5.875480047414,\n",
       "  5.875480018311263,\n",
       "  5.87547997409749,\n",
       "  5.875480029504624,\n",
       "  5.875479957867117,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.87547999368587,\n",
       "  5.8754799220483624,\n",
       "  5.8754799768958295,\n",
       "  5.875480016072591,\n",
       "  5.875480018311263,\n",
       "  5.875480040697984,\n",
       "  5.875480029504624,\n",
       "  5.875479938278737,\n",
       "  5.875480051891345,\n",
       "  5.875479941077075,\n",
       "  5.875479910855002,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.8754799948052066,\n",
       "  5.87547999368587,\n",
       "  5.875479998163215,\n",
       "  5.875480018311263,\n",
       "  5.87547999368587,\n",
       "  5.87547998249251,\n",
       "  5.875479957867117,\n",
       "  5.875479957867117,\n",
       "  5.875480011595247,\n",
       "  5.875479904138986,\n",
       "  5.87547999368587,\n",
       "  5.8754799802538376,\n",
       "  5.8754799399577395,\n",
       "  5.875480040697984,\n",
       "  5.875480033981968,\n",
       "  5.875480011595247,\n",
       "  5.875479975776493,\n",
       "  5.875480011595247,\n",
       "  5.875479957867117,\n",
       "  5.875480000401886,\n",
       "  5.875480047414,\n",
       "  5.875480029504624,\n",
       "  5.875480033981968,\n",
       "  5.8754799623444605,\n",
       "  5.875480011595247,\n",
       "  5.875480029504624,\n",
       "  5.875479969060477,\n",
       "  5.875479975776493,\n",
       "  5.875480065323377,\n",
       "  5.875480011595247,\n",
       "  5.875480029504624,\n",
       "  5.875480040697984,\n",
       "  5.87547998249251,\n",
       "  5.875480065323377,\n",
       "  5.8754799399577395,\n",
       "  5.875479975776493,\n",
       "  5.875480065323377,\n",
       "  5.8754799511511,\n",
       "  5.875479915332346,\n",
       "  5.875479957867117,\n",
       "  5.875479958986452,\n",
       "  5.875480000401886,\n",
       "  5.875479957867117,\n",
       "  5.875479933241723,\n",
       "  5.87547999368587,\n",
       "  5.875479957867117,\n",
       "  5.8754799948052066,\n",
       "  5.875479975776493,\n",
       "  5.8754799802538376,\n",
       "  5.875480029504624,\n",
       "  5.875480051891345,\n",
       "  5.875480029504624,\n",
       "  5.87548003622064,\n",
       "  5.8754799802538376,\n",
       "  5.875479975776493,\n",
       "  5.875480065323377,\n",
       "  5.8754799802538376,\n",
       "  5.875479957867117,\n",
       "  5.875480012714583,\n",
       "  5.875479957867117,\n",
       "  5.875479975776493,\n",
       "  5.875480004879231,\n",
       "  5.87547999368587,\n",
       "  5.87547998249251,\n",
       "  5.875479975776493,\n",
       "  5.875480012714583,\n",
       "  5.875480029504624,\n",
       "  5.8754800541300165,\n",
       "  5.875480047414,\n",
       "  5.875479964583133,\n",
       "  5.875479946673756,\n",
       "  5.875480029504624,\n",
       "  5.875479957867117,\n",
       "  5.875480040697984,\n",
       "  5.875480004879231,\n",
       "  5.875479964583133,\n",
       "  5.875480069800721,\n",
       "  5.875479948912428,\n",
       "  5.875479957867117,\n",
       "  5.875480029504624,\n",
       "  5.87547999368587,\n",
       "  5.875480000401886,\n",
       "  5.875480018311263,\n",
       "  5.875480033981968,\n",
       "  5.87547999368587,\n",
       "  5.8754799802538376,\n",
       "  5.875480004879231,\n",
       "  5.8754799768958295,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.875480018311263,\n",
       "  5.87547998249251,\n",
       "  5.87548003062396,\n",
       "  5.875480016072591,\n",
       "  5.875480029504624,\n",
       "  5.875479958986452,\n",
       "  5.875480029504624,\n",
       "  5.875479957867117,\n",
       "  5.875480065323377,\n",
       "  5.875479964583133,\n",
       "  5.87547999368587,\n",
       "  5.875480047414,\n",
       "  5.87547999368587,\n",
       "  5.87547998249251,\n",
       "  5.8754799511511,\n",
       "  5.875479975776493,\n",
       "  5.875480029504624,\n",
       "  5.875479957867117,\n",
       "  5.875479941077075,\n",
       "  5.87547999368587,\n",
       "  5.875480000401886,\n",
       "  5.8754799399577395,\n",
       "  5.875480016072591,\n",
       "  5.875479910855002,\n",
       "  5.875479984731181,\n",
       "  5.875479946673756,\n",
       "  5.87547999368587,\n",
       "  5.875480087710098,\n",
       "  5.87547997409749,\n",
       "  5.87547999368587,\n",
       "  5.875480011595247,\n",
       "  5.875480047414,\n",
       "  5.87547999368587,\n",
       "  5.875480012714583,\n",
       "  5.875479905258322,\n",
       "  5.875479957867117,\n",
       "  5.875480000401886,\n",
       "  5.8754799768958295,\n",
       "  5.875479946673756,\n",
       "  5.8754799802538376,\n",
       "  5.87547999368587,\n",
       "  5.875480000401886,\n",
       "  5.875479964583133,\n",
       "  5.87547998249251,\n",
       "  5.875479975776493,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.875479957867117,\n",
       "  5.875480029504624,\n",
       "  5.875480029504624,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.875479975776493,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.875480051891345,\n",
       "  5.875480029504624,\n",
       "  5.87547999368587,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.875480029504624,\n",
       "  5.875479964583133,\n",
       "  5.875480004879231,\n",
       "  5.875480000401886,\n",
       "  5.875480016072591,\n",
       "  5.875480029504624,\n",
       "  5.8754800541300165,\n",
       "  5.875480029504624,\n",
       "  5.875480047414,\n",
       "  5.875480029504624,\n",
       "  5.875479957867117,\n",
       "  5.875480016072591,\n",
       "  5.875479975776493,\n",
       "  5.875479957867117,\n",
       "  5.875479957867117,\n",
       "  5.875480047414,\n",
       "  5.8754799802538376,\n",
       "  5.875480016072591,\n",
       "  5.875479975776493,\n",
       "  5.875480011595247,\n",
       "  5.875480029504624,\n",
       "  5.875480011595247,\n",
       "  5.875479957867117,\n",
       "  5.875479957867117,\n",
       "  5.875480065323377,\n",
       "  5.87547997409749,\n",
       "  5.8754799768958295,\n",
       "  5.875479995924542,\n",
       "  5.875480004879231,\n",
       "  5.875480069800721,\n",
       "  5.875479910855002,\n",
       "  5.87547998249251,\n",
       "  5.875480040697984,\n",
       "  5.875479998163215,\n",
       "  5.87547997409749,\n",
       "  5.875480029504624,\n",
       "  5.875479904138986,\n",
       "  5.875480011595247,\n",
       "  5.875479957867117,\n",
       "  5.875479910855002,\n",
       "  5.875480029504624,\n",
       "  5.875479946673756,\n",
       "  5.875480029504624,\n",
       "  5.875480047414,\n",
       "  5.87547999368587,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.875480076516738,\n",
       "  5.8754800541300165,\n",
       "  5.8754799287643795,\n",
       "  5.875479957867117,\n",
       "  5.875480018311263,\n",
       "  5.87547999368587,\n",
       "  5.8754799399577395,\n",
       "  5.87548003062396,\n",
       "  5.875479975776493,\n",
       "  5.87547999368587,\n",
       "  5.87547998249251,\n",
       "  5.875480011595247,\n",
       "  5.87547998249251,\n",
       "  5.875480018311263,\n",
       "  5.8754799948052066,\n",
       "  5.875479975776493,\n",
       "  5.875480011595247,\n",
       "  5.875480000401886,\n",
       "  5.875480004879231,\n",
       "  5.87547999368587,\n",
       "  5.875479975776493,\n",
       "  5.875480065323377,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.875480051891345,\n",
       "  5.875480040697984,\n",
       "  5.875479910855002,\n",
       "  5.87547999368587,\n",
       "  5.8754799802538376,\n",
       "  5.875479957867117,\n",
       "  5.8754799287643795,\n",
       "  5.87547998249251,\n",
       "  5.875479975776493,\n",
       "  5.875480029504624,\n",
       "  5.87547999368587,\n",
       "  5.875479969060477,\n",
       "  5.875480016072591,\n",
       "  5.875480048533337,\n",
       "  5.875480029504624,\n",
       "  5.875480029504624,\n",
       "  5.875480029504624,\n",
       "  5.875479964583133,\n",
       "  5.875480018311263,\n",
       "  5.8754799802538376,\n",
       "  5.875479975776493,\n",
       "  5.87547999368587,\n",
       "  5.875479946673756,\n",
       "  5.875480029504624,\n",
       "  5.875480047414,\n",
       "  5.875480029504624,\n",
       "  5.875480047414,\n",
       "  5.8754799287643795,\n",
       "  5.875480020549936,\n",
       "  5.875480029504624,\n",
       "  5.875480087710098,\n",
       "  5.87547999368587,\n",
       "  5.87547999368587,\n",
       "  5.875480000401886,\n",
       "  5.875480011595247,\n",
       "  5.8754799220483624,\n",
       "  5.87547999368587,\n",
       "  5.875479957867117,\n",
       "  5.875480029504624,\n",
       "  5.87547998249251,\n",
       "  5.875480065323377,\n",
       "  5.875479975776493,\n",
       "  5.87547998249251,\n",
       "  5.8754799802538376,\n",
       "  5.875480065323377,\n",
       "  5.875480029504624,\n",
       "  5.875479984731181,\n",
       "  5.8754799399577395,\n",
       "  5.8754799287643795,\n",
       "  5.8754800541300165,\n",
       "  5.875480033981968,\n",
       "  5.875480029504624,\n",
       "  5.87547999368587,\n",
       "  5.875479969060477,\n",
       "  5.8754800541300165,\n",
       "  5.8754799948052066,\n",
       "  5.875479941077075,\n",
       "  5.875480018311263,\n",
       "  5.875480022788607,\n",
       "  5.875479902459983,\n",
       "  5.87547999368587,\n",
       "  5.875479964583133,\n",
       "  5.875480029504624,\n",
       "  5.875480011595247,\n",
       "  5.875480065323377,\n",
       "  5.875480016072591,\n",
       "  5.875479957867117,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.875480040697984,\n",
       "  5.875480048533337,\n",
       "  5.875480029504624,\n",
       "  5.875479975776493,\n",
       "  5.875479975776493,\n",
       "  5.875479957867117,\n",
       "  5.87547999368587,\n",
       "  5.8754799399577395,\n",
       "  5.875480047414,\n",
       "  5.875479935480396,\n",
       "  5.8754799802538376,\n",
       "  5.875479957867117,\n",
       "  5.875480012714583,\n",
       "  5.875480029504624,\n",
       "  5.875480051891345,\n",
       "  5.875480048533337,\n",
       "  5.8754799220483624,\n",
       "  5.875479957867117,\n",
       "  5.875480000401886,\n",
       "  5.875480009916244,\n",
       "  5.87547999368587,\n",
       "  5.875480065323377,\n",
       "  5.87547999368587,\n",
       "  5.875480051891345,\n",
       "  5.875480029504624,\n",
       "  5.875480029504624,\n",
       "  5.875480011595247,\n",
       "  5.875480065323377,\n",
       "  5.875480047414,\n",
       "  5.875479957867117,\n",
       "  5.875480016072591,\n",
       "  5.875479946673756,\n",
       "  5.875480011595247,\n",
       "  5.875480016072591,\n",
       "  5.875479937719067,\n",
       "  5.875480065323377,\n",
       "  5.875479872797577,\n",
       "  5.875480004879231,\n",
       "  5.875480029504624,\n",
       "  5.875479946673756,\n",
       "  5.875480029504624,\n",
       "  5.875480011595247,\n",
       "  5.87547997409749,\n",
       "  5.875479975776493,\n",
       "  5.875480065323377,\n",
       "  5.875479975776493,\n",
       "  5.875480029504624,\n",
       "  5.8754799399577395,\n",
       "  5.875480000401886,\n",
       "  5.875480022788607,\n",
       "  5.875480011595247,\n",
       "  5.875480011595247,\n",
       "  5.875480029504624,\n",
       "  5.875480029504624,\n",
       "  5.875479957867117,\n",
       "  5.875479957867117,\n",
       "  5.875480011595247,\n",
       "  5.875479975776493,\n",
       "  5.875480029504624,\n",
       "  5.875480047414,\n",
       "  5.875479969060477,\n",
       "  5.87547999368587,\n",
       "  5.875480012714583,\n",
       "  5.875479975776493,\n",
       "  5.875480000401886,\n",
       "  5.87547999368587,\n",
       "  5.875480029504624,\n",
       "  5.875479946673756,\n",
       "  5.875480016072591]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss=pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x292139b8f28>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(units=30,activation=\"relu\"))\n",
    "model.add(Dense(units=15,activation=\"relu\"))\n",
    "model.add(Dense(units=1,activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop training when amonitored quantity has stopped imporving arguments arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop=EarlyStopping(monitor=\"val_loss\",mode=\"min\",verbose=1,patience=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 1s 2ms/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 65us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 50us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 52us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 44us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 44us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 49us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 58us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 58us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 52us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 66us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 52us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 47us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 48us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 45us/step - loss: 5.8755 - val_loss: 6.1317\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29213a1af60>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,\n",
    "         y=y_train,\n",
    "         epochs=600,\n",
    "         validation_data=(X_test,y_test),verbose=1,\n",
    "         callbacks=[early_stop]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x292101b87f0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss=pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding in dropout layers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model=Sequential()\n",
    "model.add(Dense(units=30,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=15,activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=1,activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 1s 2ms/step - loss: 7.6678 - val_loss: 6.1317\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 90us/step - loss: 8.0690 - val_loss: 6.1317\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 59us/step - loss: 6.8700 - val_loss: 6.1317\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 7.5439 - val_loss: 6.1317\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 7.7114 - val_loss: 6.1317\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 6.7662 - val_loss: 6.1317\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 7.3993 - val_loss: 6.1317\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 6.9811 - val_loss: 6.1317\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 58us/step - loss: 7.2962 - val_loss: 6.1317\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 6.6055 - val_loss: 6.1317\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 64us/step - loss: 6.9360 - val_loss: 6.1317\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 57us/step - loss: 6.8127 - val_loss: 6.1317\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 7.3626 - val_loss: 6.1317\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 95us/step - loss: 7.2265 - val_loss: 6.1317\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 80us/step - loss: 6.1493 - val_loss: 6.1317\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 60us/step - loss: 7.2436 - val_loss: 6.1317\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 68us/step - loss: 6.5992 - val_loss: 6.1317\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 7.0502 - val_loss: 6.1317\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 55us/step - loss: 7.2204 - val_loss: 6.1317\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 51us/step - loss: 6.8607 - val_loss: 6.1317\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 56us/step - loss: 6.9845 - val_loss: 6.1317\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 6.1288 - val_loss: 6.1317\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 62us/step - loss: 6.6627 - val_loss: 6.1317\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 53us/step - loss: 6.3354 - val_loss: 6.1317\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 54us/step - loss: 5.9667 - val_loss: 6.1317\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 62us/step - loss: 6.8160 - val_loss: 6.1317\n",
      "Epoch 00026: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x292142da0f0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,\n",
    "         y=y_train,\n",
    "         epochs=600,\n",
    "         validation_data=(X_test,y_test),verbose=1,\n",
    "         callbacks=[early_stop]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x292149c8710>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss=pd.DataFrame(model.history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e8606da63845>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# convert probabilities to class labels (0 or 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_prob\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_prob' is not defined"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "# get probabilities\n",
    "\n",
    "# convert probabilities to class labels (0 or 1)\n",
    "predictions = (y_prob > 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
