{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#create a tiny toy dataset\n",
    "#we want the network to learn: y=2x+1\n",
    "\n",
    "x=np.array([[-1,0],\n",
    "           ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#network architecture\n",
    "#-1 - input neuron\n",
    "#-2 - hidden layer neurons\n",
    "#-1 - output neuron\n",
    "#hidden activation=sigmoid\n",
    "#loss function = mean squared error\n",
    "\n",
    "np.random.seed(0)\n",
    "in_dim=1\n",
    "hidden_dim=2\n",
    "out_dim=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight and bias initaialization\n",
    "#small random weights + zero biases\n",
    "\n",
    "W1=np.random.randn(in_dim.hidden_dim) * 0.1\n",
    "b1=np.zeros(1,hidden_dim)\n",
    "\n",
    "W2=np.random.randn(hidden_dim,out_dim)\n",
    "b2=np.zeros(1,out_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid activation and derivative \n",
    "def sigmoid(z):\n",
    "    return 1/ (1+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    s = sigmoid(z)\n",
    "    return s*(1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n",
    "lr=0.1\n",
    "epochs=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean squared error\n",
    "def mse(y_true,y_pred):\n",
    "    return np.mean((y_true,y_pred)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop - forward +backword\n",
    "\n",
    "for epochs in range (1,epochs+1):\n",
    "    \n",
    "    #hidden layer linear combination: z1=xw1+b1\n",
    "    z1=x @W1 + b1\n",
    "    \n",
    "    #apply sigmoid activation\n",
    "    a1=sigmoid(z1)\n",
    "    \n",
    "    #output layers : z2=a1W1 + b2\n",
    "    z2= a1 @ w2 + b2 \n",
    "    y_pred = z2\n",
    "    \n",
    "    # 2.COMPUTE LOSS\n",
    "    loss=mse(y,y_pred)\n",
    "    \n",
    "    \n",
    "    # 3.BACKWORD PASS\n",
    "    \n",
    "    N=x.shape[0]\n",
    "    #x=[rows,columns] x [0] = no of rows\n",
    "    #gradient of mse w..r.t predictions\n",
    "    #\n",
    "    dl_dy=(2.0 / N) * (y_pred - y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dW2=a1.T @ dl_dy\n",
    "    \n",
    "    #db2=np.sum(dl_dy,axis=0,keepdims=True)\n",
    "    db2=np.sum(dl_dy,axis=0,keepdims=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #backpropogate into hiddenlayers\n",
    "    \n",
    "    #delta_hidden=dl/dz1\n",
    "    delta_hidden=(dl_dy @ W2.T) * sigmoid_prime(z1)\n",
    "    \n",
    "    #gradient for w1 and b1\n",
    "    dW1=x.T @ delta_hidden\n",
    "    \n",
    "    #db1=row-wise sum\n",
    "    db1=no.sum(delta_hidden,axis=0,keepdims=True)\n",
    "    \n",
    "    \n",
    "    # UPDATE WEIGHTS\n",
    "    \n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    \n",
    "    \n",
    "    #print loss occasionalyy\n",
    "    if epoch % 20 == 0 or epoch == 1:\n",
    "        print(f\"epoch {epoch:3d} loss = {loss:.6f})\n",
    "    \n",
    "    #this code prints the loss value every 2o epochs\n",
    "    \n",
    "              \n",
    "    #after training:show predictions\n",
    "    print(\"\\nFinal predictions vs actaul predictions\")\n",
    "              \n",
    "    z1= X @ W1 + b1\n",
    "    a1=sigmoid(z1)\n",
    "    y_pred=a1 @ W2 + b2\n",
    "    \n",
    "              \n",
    "    for xi,yi,ypi in zip (x.flatten(),y.flatten(),y_pred.flatten()):\n",
    "              print(f\"X={xi: .2f} \n",
    "\n",
    "    z1="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
